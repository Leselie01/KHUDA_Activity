{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4EEy9KDUADb"
      },
      "source": [
        "Q1. chapter3에서 살펴본 기존 CBOW모델의 문제점 2가지는 무엇인가요?\n",
        "\n",
        "- 1. 어휘 수가 많아지면 입력층의 원핫 표현의 벡터 크기가 커지고, 이에 따라 많은 계산이 필요하게 된다.\n",
        "- 2. 어휘가 많아지게 되면 은닉층과 가중치 행력의 곱 & Softmax 계층의 계산이 증가한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSV10Ja8U23p"
      },
      "source": [
        "Q2. Embedding 계층이란 무엇인가요?\n",
        "- 답 : 가중치 매개변수로부터 '단어 ID에 해당하는 행(벡터)'을 추출하는 계층"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyogYswhVf_m"
      },
      "source": [
        "Q3. Embedding 계층을 Class형태로 구현해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SxBdNEi2Ts9m"
      },
      "outputs": [],
      "source": [
        "# 아래에 코드를 작성해주세요.\n",
        "class Embedding:\n",
        "  def __init__(self, W):\n",
        "    self.params = [W]\n",
        "    self.grads = [np.zeros_like(W)]\n",
        "    self.idx = None\n",
        "\n",
        "  def forward(self, idx):\n",
        "    W, = self.params\n",
        "    self.idx = idx\n",
        "    out = W[idx]\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dW, = self.grads\n",
        "    dW[...] = 0\n",
        "    if GPU:\n",
        "      import cupyx\n",
        "      cupyx.scatter_add(dW, self.idx, dout)\n",
        "    else:\n",
        "      np.add.at(dW, self.idx, dout)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2-DyNepVluv"
      },
      "source": [
        "Q4. Negative Sampling이란 무엇인가요?\n",
        "\n",
        "- 답 :  word2vec을 만들 때 현재 문장에 없는 단어를 전체 데이터 셋에서 추출하는 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCSQMgoZWC6p"
      },
      "source": [
        "Q5. 다중 분류와 이중 분류에서 각각의 활성화 함수와 손실 함수는 무엇이고, 왜 그런 함수를 사용하는지 적어주세요.\n",
        "\n",
        "- 답 : 이진 분류와 다중 분류 모두 손실 함수로 '교차 엔트로피 오차'를 사용한다. 활성화 함수로는 이진 분류는 Sigmoid 함수를, 다중 분류는 Softmax 함수를 사용한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUXCLtCvXT3e"
      },
      "source": [
        "Q6. Negative Sampling을 구현해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3shdQo1pfM9",
        "outputId": "360254ff-15c1-4d27-e8b1-1bc271832836"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GoYI9wjXW0z"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.np import *  # import numpy as np\n",
        "from common.layers import Embedding, SigmoidWithLoss\n",
        "import collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "exYNO-EWrq6O"
      },
      "outputs": [],
      "source": [
        "class EmbeddingDot:\n",
        "  def __init__(self, W):\n",
        "    self.embed = Embedding(W)\n",
        "    self.params = self.embed.params\n",
        "    self.grads = self.embed.grads\n",
        "    self.cache = None\n",
        "  \n",
        "  def forward(self, h, idx):\n",
        "    target_W = self.embed.forward(idx)\n",
        "    out = np.sum(target_W * h, axis = 1)\n",
        "\n",
        "    self.cache = (h, target_W)\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    h, target_W = self.cache\n",
        "    dout = dout.reshape(dout.shape[0], 1)\n",
        "\n",
        "    dtarget_W = dout * h\n",
        "    self.embed.backward(dtarget_W)\n",
        "    dh = dout * target_W\n",
        "    return dh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lyFMNvynrPoN"
      },
      "outputs": [],
      "source": [
        "class UnigramSampler:\n",
        "    def __init__(self, corpus, power, sample_size):\n",
        "        self.sample_size = sample_size\n",
        "        self.vocab_size = None\n",
        "        self.word_p = None\n",
        "        \n",
        "        counts = collections.Counter()\n",
        "        for word_id in corpus:\n",
        "            counts[word_id] += 1\n",
        "            \n",
        "        vocab_size = len(counts)\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        self.word_p = np.zeros(vocab_size)\n",
        "        for i in range(vocab_size):\n",
        "            self.word_p[i] = counts[i]\n",
        "            \n",
        "        self.word_p = np.power(self.word_p, power)\n",
        "        self.word_p /= np.sum(self.word_p)\n",
        "        \n",
        "    def get_negative_sample(self, target):\n",
        "        batch_size = target.shape[0]\n",
        "        \n",
        "        if not GPU:\n",
        "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
        "            \n",
        "            for i in range(batch_size):\n",
        "                p = self.word_p.copy()\n",
        "                target_idx = target[i]\n",
        "                p[target_idx] = 0 \n",
        "                p /= p.sum()  # 다시 정규화\n",
        "                negative_sample[i, :] = np.random.choice(self.vocab_size,\n",
        "                                                         size=self.sample_size,\n",
        "                                                         replace=False, p=p)\n",
        "                \n",
        "        else:\n",
        "            # GPU(cupy)로 계산할 때는 속도를 우선한다.\n",
        "            # 부정적 예에 타깃이 포함될 수 있다.\n",
        "            negative_sample = np.random.choice(self.vocab_size, \n",
        "                                               size=(batch_size, self.sample_size), \n",
        "                                               replace=True, p=self.word_p)\n",
        "            \n",
        "        return negative_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "l3Zezq-CXbg0"
      },
      "outputs": [],
      "source": [
        "class NegativeSamplingLoss:\n",
        "  def __init__(self, W, corpus, power = 0.75, sample_size = 5):\n",
        "      self.sample_size = sample_size\n",
        "      self.sampler = UnigramSampler(corpus, power, sample_size)\n",
        "      self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
        "      self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
        "      self.params, self.grads = [], []\n",
        "      for layer in self.embed_dot_layers:\n",
        "        self.params += layer.params\n",
        "        self.grads += layer.grads\n",
        "\n",
        "  def forward(self, h, target):\n",
        "      batch_size = target.shape[0]\n",
        "      negative_sample = self.sampler.get_negative_sample(target)\n",
        "\n",
        "      #긍정적 예 순전파\n",
        "      score = self.embed_dot_layers[0].forward(h, target)\n",
        "      correct_label = np.ones(batch_size, dtype = np.int32)\n",
        "      loss = self.loss_layers[0].forward(score, correct_label)\n",
        "\n",
        "      #부정적 예 순전파\n",
        "      negative_label = np.zeros(batch_size, dtype = np.int32)\n",
        "      for i in range(self.sample_size):\n",
        "        negative_target = negative_sample[:, i]\n",
        "        score = self.embed_dot_layers[1+i].forward(h, negative_target)\n",
        "        loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
        "\n",
        "  def backward( self, dout=1): \n",
        "      dh = 0\n",
        "      for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
        "          dscore = l0 .backward(dout) \n",
        "          dh += l1 .backward(dscore)\n",
        "\n",
        "      return dh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTTCIq2pXqf3"
      },
      "source": [
        "Q7. 위 Embedding 계층과 Negative Sampling을 추가한 개선된 CBOW 모델을 Class형태로 구현해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6mMsI-AAX1Ib"
      },
      "outputs": [],
      "source": [
        "# chap04/cbow.py\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.np import *\n",
        "from common.layers import Embedding\n",
        "from negative_sampling_layer import NegativeSamplingLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lm3tciYzX2ak"
      },
      "outputs": [],
      "source": [
        "# 아래에 코드를 작성해주세요.\n",
        "class CBOW : \n",
        "    def __init__(self, vocab_size, hidden_size, window_size, corpus): \n",
        "        V, H = vocab_size, hidden_size \n",
        "\n",
        "        # 가중치 초기화 \n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f') \n",
        "        W_out = 0.01 * np.random.randn(V, H).astype ('f') \n",
        "\n",
        "        # 계층 생성 \n",
        "        self.in_layers = [ ] \n",
        "        for i in range (2 * window_size): \n",
        "            layer = Embedding(W_in)           # Embedding 계층 사용 \n",
        "            self.in_layers.append(layer) \n",
        "        self.ns_loss = NegativeSamplingloss(W_out ,corpus, power = 0.75, sampe_size = 5)\n",
        "\n",
        "        # 모든 가중치와 기울기를 배열에 모은다. \n",
        "        layers = self.in_layers + [self.ns_loss] \n",
        "        self.params, self.grads = [ ], [ ] \n",
        "        for layer in layers: \n",
        "            self.params += layer.params \n",
        "            self.grads += layer.grads\n",
        "\n",
        "        # 인스턴스 변수에 단어의 분산 표현을 저장한다 . \n",
        "        self.word_vecs = W_in\n",
        "\n",
        "    def forward (self, contexts, target): \n",
        "        h = 0 \n",
        "        for i, layer in enumerate (self.in_layers): \n",
        "            h += layer.forward(contexts[:, i]) \n",
        "        h *= 1 / len(self.in_layers) \n",
        "        loss = self.ns_loss.forward(h, target) \n",
        "        return loss \n",
        "    def backward (self, dout=1): \n",
        "        dout = self.ns_loss.backward(dout) \n",
        "        dout *= 1 / len (self.in_layers)\n",
        "        for layer in self.in_layers: \n",
        "            layer.backward(dout) \n",
        "        return None \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5TBFt2HYEIo"
      },
      "source": [
        "Q8. CBOW Model을 학습하여 파라미터를 pkl파일로 저장해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0F6hQHGjYULT"
      },
      "outputs": [],
      "source": [
        "# chap04/train.py\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common import config\n",
        "# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).\n",
        "# ===============================================\n",
        "# config.GPU = True\n",
        "# ===============================================\n",
        "import pickle\n",
        "from common.trainer import Trainer\n",
        "from common.optimizer import Adam\n",
        "from cbow import CBOW\n",
        "from skip_gram import SkipGram\n",
        "from common.util import create_contexts_target, to_cpu, to_gpu\n",
        "from dataset import ptb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cMAhn9BYYiw8"
      },
      "outputs": [],
      "source": [
        "# 하이퍼파라미터 설정\n",
        "window_size = 5\n",
        "hidden_size = 100\n",
        "batch_size = 100\n",
        "max_epoch = 10\n",
        "\n",
        "# 데이터 읽기\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "if config.GPU:\n",
        "    contexts, target = to_gpu(contexts), to_gpu(target)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5fA5vlbJYoH-"
      },
      "outputs": [],
      "source": [
        "# 모델 등 생성\n",
        "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "s3aaYb67YpuX",
        "outputId": "0e85c5fd-251a-49f4-e29d-eeba00d5b4bf"
      },
      "outputs": [],
      "source": [
        "# 학습 시작\n",
        "trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "trainer.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fP083OHHYs8m"
      },
      "outputs": [],
      "source": [
        "# pkl 파일 저장\n",
        "word_vecs = model.word_vecs\n",
        "if config.GPU:\n",
        "    word_vecs = to_cpu(word_vecs)\n",
        "params = {}\n",
        "params['word_vecs'] = word_vecs.astype(np.float16)\n",
        "params['word_to_id'] = word_to_id\n",
        "params['id_to_word'] = id_to_word\n",
        "pkl_file = 'cbow_params.pkl'\n",
        "with open(pkl_file, 'wb') as f:\n",
        "    pickle.dump(params, f, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WK2Jhw7Y5_N"
      },
      "source": [
        "Q9. pkl파일을 불러와 예시 query를 작성해 결과를 보고, 유추(analogy)작업을 수행해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IpDAIIHYZLC-"
      },
      "outputs": [],
      "source": [
        "# pkl 파일 불러오기\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.util import most_similar, analogy\n",
        "import pickle\n",
        "\n",
        "pkl_file = 'cbow_params.pkl'\n",
        "# pkl_file = 'skipgram_params.pkl'\n",
        "\n",
        "with open(pkl_file, 'rb') as f:\n",
        "    params = pickle.load(f)\n",
        "    word_vecs = params['word_vecs']\n",
        "    word_to_id = params['word_to_id']\n",
        "    id_to_word = params['id_to_word']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iNHE5pogdK1m"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[query] you\n",
            " we: 0.72705078125\n",
            " i: 0.7080078125\n",
            " your: 0.6142578125\n",
            " someone: 0.59521484375\n",
            " they: 0.591796875\n",
            "\n",
            "[query] year\n",
            " month: 0.86767578125\n",
            " week: 0.78564453125\n",
            " summer: 0.76416015625\n",
            " spring: 0.744140625\n",
            " decade: 0.6884765625\n",
            "\n",
            "[query] car\n",
            " luxury: 0.62109375\n",
            " window: 0.60302734375\n",
            " cars: 0.595703125\n",
            " auto: 0.59033203125\n",
            " truck: 0.57763671875\n",
            "\n",
            "[query] toyota\n",
            " coated: 0.62255859375\n",
            " honda: 0.5947265625\n",
            " chevrolet: 0.59423828125\n",
            " z: 0.58935546875\n",
            " ford: 0.58544921875\n"
          ]
        }
      ],
      "source": [
        "# 예시 query 작성 후 비슷한 단어 살펴보기\n",
        "querys = ['you', 'year', 'car', 'toyota']\n",
        "for query in querys:\n",
        "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5-RKMJ1MdNzJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "\n",
            "[analogy] king:man = queen:?\n",
            " a.m: 5.328125\n",
            " woman: 4.86328125\n",
            " naczelnik: 4.8515625\n",
            " carolinas: 4.51171875\n",
            " wife: 4.3828125\n",
            "\n",
            "[analogy] take:took = go:?\n",
            " went: 4.37109375\n",
            " came: 4.23828125\n",
            " 're: 4.1953125\n",
            " were: 4.13671875\n",
            " eurodollars: 4.08984375\n",
            "\n",
            "[analogy] car:cars = child:?\n",
            " a.m: 5.86328125\n",
            " rape: 5.75\n",
            " children: 5.0703125\n",
            " adults: 4.71484375\n",
            " incest: 4.6640625\n",
            "\n",
            "[analogy] good:better = bad:?\n",
            " rather: 5.421875\n",
            " more: 5.33203125\n",
            " less: 5.20703125\n",
            " greater: 3.890625\n",
            " fewer: 3.748046875\n"
          ]
        }
      ],
      "source": [
        "# 유추(analogy) 작업 수행\n",
        "print('-'*50)\n",
        "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\n",
        "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW_vymP_Zh6m"
      },
      "source": [
        "Q10. 개선전 CBOW모델을 동일한 데이터셋으로 학습하여 나온 결과를 개선된 CBOW모델의 학습 결과와 비교해서 분석해주세요. (시간, 학습 결과 등)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EJilgrKRZxWT"
      },
      "outputs": [],
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.layers import MatMul, SoftmaxWithLoss\n",
        "\n",
        "\n",
        "class SimpleCBOW:\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        V, H = vocab_size, hidden_size\n",
        "\n",
        "        # 가중치 초기화\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
        "\n",
        "        # 계층 생성\n",
        "        self.in_layer0 = MatMul(W_in)\n",
        "        self.in_layer1 = MatMul(W_in)\n",
        "        self.out_layer = MatMul(W_out)\n",
        "        self.loss_layer = SoftmaxWithLoss()\n",
        "\n",
        "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
        "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
        "        self.word_vecs = W_in\n",
        "\n",
        "    def forward(self, contexts, target):\n",
        "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
        "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
        "        h = (h0 + h1) * 0.5\n",
        "        score = self.out_layer.forward(h)\n",
        "        loss = self.loss_layer.forward(score, target)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        ds = self.loss_layer.backward(dout)\n",
        "        da = self.out_layer.backward(ds)\n",
        "        da *= 0.5\n",
        "        self.in_layer1.backward(da)\n",
        "        self.in_layer0.backward(da)\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('..')  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "from common.trainer import Trainer\n",
        "from common.optimizer import Adam\n",
        "from simple_cbow import SimpleCBOW\n",
        "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
        "\n",
        "window_size = 5\n",
        "hidden_size = 100\n",
        "batch_size = 100\n",
        "max_epoch = 10\n",
        "\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "vocab_size = len(word_to_id)\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "target = convert_one_hot(target, vocab_size)\n",
        "contexts = convert_one_hot(contexts, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "929587"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model = SimpleCBOW(vocab_size, hidden_size)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "trainer.plot()\n",
        "\n",
        "word_vecs = model.word_vecs\n",
        "for word_id, word in id_to_word.items():\n",
        "    print(word, word_vecs[word_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "92만개 데이터로 인하여 계산량 증가로 torch가 죽는 현상이 발생됨, 이로써 이전 CBOW보다 지금 CBOW가 성능면에서 좋다고 볼 수 있다."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "KHUDA과제_2_최예지.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
