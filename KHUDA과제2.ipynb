{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W80eKUAP9_BQ"
      },
      "source": [
        "# 코드 따라쓰기\n",
        "##### 첫 시간에 받은 common 폴더의 time_layers.py를 이용하겠습니다. np.py, layers.py, functions.py 등이 함께 사용됩니다\n",
        "##### time_layers를 구현할 때 **RNN 관련 코드를 지우고** 다시 써보면서 한 줄씩 숙지해봅시다.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPh-P7c99_BY"
      },
      "source": [
        "```bash\n",
        "Chap 05\n",
        "├── common\n",
        "│   ├── time_layers.py\n",
        "│   ├── np.py\n",
        "│   ├── layers.py\n",
        "│   └── etc...\n",
        "├── KHUDA_Homework3_홍길동.py\n",
        "├── simple_rnnlm.py\n",
        "└── train.py\n",
        "``` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSeqPiib9_Bc"
      },
      "source": [
        "##### 1. RNN Class를 구현해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyobLBvA9_Bg"
      },
      "outputs": [],
      "source": [
        "# common/time_layers.py\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.np import *\n",
        "from common.layers import *\n",
        "from common.functions import sigmoid\n",
        "\n",
        "\n",
        "class RNN:\n",
        "    def __init__(self, Wx, Wh, b):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        Wx, Wh, b = self.params\n",
        "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
        "        h_next = np.tanh(t)\n",
        "\n",
        "        self.cache = (x, h_prev, h_next)\n",
        "        return h_next\n",
        "\n",
        "    def backward(self, dh_next):\n",
        "        Wx, Wh, b = self.params\n",
        "        x, h_prev, h_next = self.cache\n",
        "\n",
        "        dt = dh_next * (1 - h_next ** 2)\n",
        "        db = np.sum(dt, axis=0)\n",
        "        dWh = np.dot(h_prev.T, dt)\n",
        "        dh_prev = np.dot(dt, Wh.T)\n",
        "        dWx = np.dot(x.T, dt)\n",
        "        dx = np.dot(dt, Wx.T)\n",
        "\n",
        "        self.grads[0][...] = dWx\n",
        "        self.grads[1][...] = dWh\n",
        "        self.grads[2][...] = db\n",
        "\n",
        "        return dx, dh_prev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRg6n1rG9_Bi"
      },
      "source": [
        "##### 2. TimeRNN Class를 구현해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nsk6HOZ09_Bj"
      },
      "outputs": [],
      "source": [
        "# common/time_layers.py\n",
        "class TimeRNN:\n",
        "    def __init__(self, Wx, Wh, b, stateful=False):\n",
        "        self.params = [Wx, Wh, b]\n",
        "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
        "        self.layers = None  # RNN 계층을 리스트로 저장\n",
        "        \n",
        "        self.h, self.dh = None, None\n",
        "        self.stateful = stateful\n",
        "\n",
        "    def set_state(self,h):\n",
        "        self.h = h\n",
        "    \n",
        "    def reset_state(self):\n",
        "        self.h = None\n",
        "    \n",
        "    def forward(self, xs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, D = xs.shape\n",
        "        D, H = Wx.shape\n",
        "\n",
        "        self.laters = []\n",
        "        hs = np.empty((N,T,H), dtype = 'f')\n",
        "\n",
        "        if not self.stateful or self.h is None:\n",
        "            self.h = np.zeros((N, H), dtype = 'f')\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = RNN(*self.param)\n",
        "            self.h = layer.forward(xs[:,t, :], self.h)\n",
        "            hs[:,t,:] = self.h\n",
        "            self.layers.append(layer)\n",
        "        \n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        Wx, Wh, b = self.params\n",
        "        N, T, H = dhs.shape\n",
        "        D, H = Wx.shape\n",
        "\n",
        "        dxs = np.empty((N, T, D), dtpye = 'f')\n",
        "        dh = 0\n",
        "        grads = [0, 0, 0]\n",
        "        for t in reversed(range(T)):\n",
        "            layer = self.layers[t]\n",
        "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
        "            dxs[:, t, :] = dx\n",
        "\n",
        "            for i, grad in enumerate(layer.grads):\n",
        "                grads[i] += grad\n",
        "        \n",
        "        for i, grad in enumerate(grads):\n",
        "            self.grads[i][...] = grad\n",
        "        self.dh = dh \n",
        "\n",
        "        return dxs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNdBOvxP9_Bk"
      },
      "source": [
        "##### 3.TimeEmbedding Class를 구현해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJKp815n9_Bm"
      },
      "outputs": [],
      "source": [
        "# common/time_layers.py\n",
        "class TimeEmbedding:\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.layers = None\n",
        "        self.W = W\n",
        "\n",
        "    def forward(self, xs):\n",
        "        N, T = xs.shape\n",
        "        V, D = self.W.shape\n",
        "\n",
        "        out = np.empty((N, T, D), dtype = 'f')\n",
        "        self.layers = []\n",
        "\n",
        "        for t in range(T):\n",
        "            layer = Embedding(self.W) # <- layer.py에서 가져옴\n",
        "            out[:, t, :] = layer.forward(xs[:,t])\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        N, T, D = dout.shape\n",
        "        grad = 0\n",
        "        \n",
        "        for t in range(T):\n",
        "            layer = self.layers[t]\n",
        "            layer.backward(dout[:, t, :])\n",
        "            grad += layer.grads[0]\n",
        "\n",
        "        self.grads[0][...] = grad\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwmfXgrs9_Bn"
      },
      "source": [
        "##### 4.TimeAffine Class를 구현해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EarxcsZV9_Bo"
      },
      "outputs": [],
      "source": [
        "# common/time_layers.py\n",
        "class TimeAffine:\n",
        "    def __init__(self, W, b):\n",
        "        self.params = [W, b]\n",
        "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, T, D = x.shape\n",
        "        W, b = self.params\n",
        "        \n",
        "        rx = x.reshape(N*T, -1)\n",
        "        out = np.dot(rx, W) + b\n",
        "        self.x = x\n",
        "        return out.reshape(N, T, -1)\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        x = self.x\n",
        "        N, T, D = x.shape\n",
        "        W, b = self.params\n",
        "\n",
        "        dout = dout.reshape(N*T, -1)\n",
        "        rx = x.reshape(N*T, -1)\n",
        "\n",
        "        db = np.sum(dout, axis = 0)\n",
        "        dW = np.dot(rx.T, dout)\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dx = dx.reshape(*x.shape)\n",
        "\n",
        "        self.grads[0][...] = dW\n",
        "        self.grads[1][...] = db\n",
        "        \n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5567tINs9_Bp"
      },
      "source": [
        "##### 5.TimeSoftmaxWithLoss Class를 구현해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8W2pbWwv9_Bq"
      },
      "outputs": [],
      "source": [
        "# common/time_layers.py\n",
        "class TimeSoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.cache = None\n",
        "        self.ignore_label = -1\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        N, T, V = xs.shape\n",
        "\n",
        "        if ts.ndim == 3:\n",
        "            ts = ts.argmax(axis = 2)\n",
        "\n",
        "        mask = (ts != self.ignore_label)\n",
        "\n",
        "        xs = xs.reshape(N*T, V)\n",
        "        ts = ts.reshape(N*T)\n",
        "        mask = mask.reshape(N*T)\n",
        "\n",
        "        ys = softmax(xs)\n",
        "        ls = np.log(ys[np.arange(N*T), ts])\n",
        "        ls *= mask\n",
        "        loss = -np.sum(ls)\n",
        "        loss /= mask.sum()\n",
        "\n",
        "        self.cache = (ts, ys, mask, (N, T, V))\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout = 1):\n",
        "        ts, ys, mask, (N, T,V) = self.cache\n",
        "        dx = ys\n",
        "        dx[np.arange(N * T), ts] -= 1\n",
        "        dx *= dout\n",
        "        dx /= mask.sum()\n",
        "        dx *= mask[:, np.newaxis]\n",
        "\n",
        "        dx = dx.reshape((N, T, V))\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnKND4639_Bq"
      },
      "source": [
        "##### 6. SimpleRnnlm Class를 구현해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUNZONcV9_Bq"
      },
      "outputs": [],
      "source": [
        "# chap05/simple_rnnlm.py\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from common.time_layers import *\n",
        "\n",
        "class SimpleRnnlm:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
        "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
        "        rnn_b = np.zeros(H).astype('f')\n",
        "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.layers = [\n",
        "            TimeEmbedding(embed_W),\n",
        "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful = True),\n",
        "            TimeAffine(affine_W, affine_b),\n",
        "        ]\n",
        "        self.loss_layer = TimeSoftmaxWithLoss()\n",
        "        self.rnn_layer = self.layers[1]\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in self.layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "        \n",
        "    def forward(self, xs, ts):\n",
        "        for layer in self.layers:\n",
        "            xs = layer.forward(xs)\n",
        "        loss = self.loss_layer.forward(xs, ts)\n",
        "        return loss\n",
        "    \n",
        "    def backward(self, dout = 1):\n",
        "        dout = self.loss_layer.backward(dout)\n",
        "        for layer in reversed(self.layers):\n",
        "            dout = layer.backward(dout)\n",
        "        return dout\n",
        "    \n",
        "    def reset_state(self):\n",
        "        self.rnn_layer.reset_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74sWkF4P9_Br"
      },
      "source": [
        "##### 7. 모델을 학습합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1DEFURK9_Br"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from common.optimizer import SGD\n",
        "from dataset import ptb\n",
        "from simple_rnnlm import SimpleRnnlm\n",
        "\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "batch_size = 10\n",
        "wordvec_size = 100\n",
        "hidden_size = 100\n",
        "time_size = 5\n",
        "lr = 0.1\n",
        "max_epoch = 100\n",
        "\n",
        "# 학습 데이터 읽기(전체 중 1000개만)\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "corpus_size = 1000\n",
        "corpus = corpus[:corpus_size]\n",
        "vocab_size = int(max(corpus) + 1)\n",
        "\n",
        "xs = corpus[:-1]\n",
        "ts = corpus[1: ]\n",
        "data_size = len(xs)\n",
        "print('말뭉치 크기 %d, 어휘 수: %d' % (corpus_size, vocab_size))\n",
        "\n",
        "# 학습 시 사용하는 변수\n",
        "max_iters = data_size // (batch_size * time_size)\n",
        "time_idx = 0\n",
        "total_loss = 0\n",
        "loss_count = 0\n",
        "ppl_list = []\n",
        "\n",
        "# 모델 생성\n",
        "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = SGD(lr)\n",
        "\n",
        "\n",
        "# 미니배치의 각 샘플의 읽기 시작 위치를 계산\n",
        "jump = (corpus_size - 1)// batch_size\n",
        "offsets = [i * jump for i in range(batch_size)]\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    for iter in range(max_iters):\n",
        "        # 미니배치 취득\n",
        "        batch_x = np.empty((batch_size, time_size), dtype = 'i')\n",
        "        batch_t = np.empty((batch_size, time_size), dtype = 'i')\n",
        "        for t in range(time_size):\n",
        "            for i, offset in enumerate(offsets):\n",
        "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
        "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
        "            time_idx += 1\n",
        "        # 기울기를 구하여 매개변수 갱신\n",
        "        loss = model.forward(batch_x, batch_t)\n",
        "        model.backward()\n",
        "        optimizer.update(model.params, model.grads)\n",
        "        total_loss += loss\n",
        "        loss_count += 1\n",
        "\n",
        "    # 에폭마다 퍼플렉서티 평가\n",
        "    ppl = np.exp(total_loss / loss_count)\n",
        "    print('| 에폭 %d | 퍼플렉서티 %.2f' % (epoch+1, ppl))\n",
        "    ppl_list.append(float(ppl))\n",
        "    total_loss, loss_count = 0, 0\n",
        "    \n",
        "# 그래프 그리기\n",
        "x = np.arange(len(ppl_list))\n",
        "plt.plot(x, ppl_list, label = 'train')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('perplexity')\n",
        "plt.show()\n",
        "\n",
        "# 그래프 그리기\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad0VlFI59_Br"
      },
      "source": [
        "##### 8. train.py로 코드를 분리해 모델을 학습합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e20YDhD9_Bs"
      },
      "outputs": [],
      "source": [
        "# chap05/train.py\n",
        "%matplotlib inline\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.optimizer import SGD\n",
        "from common.trainer import RnnlmTrainer\n",
        "from dataset import ptb\n",
        "from simple_rnnlm import SimpleRnnlm\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "batch_size = 10\n",
        "wordvec_size = 100\n",
        "hidden_size = 100\n",
        "time_size = 5\n",
        "lr = 0.1\n",
        "max_epoch = 100\n",
        "\n",
        "# 학습 데이터 읽기\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
        "corpus_size = 1000\n",
        "corpus = corpus[:corpus_size]\n",
        "vocab_size = int(max(corpus) + 1)\n",
        "xs = corpus[:-1]\n",
        "ts = corpus[1:]\n",
        "\n",
        "# 모델 생성\n",
        "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = SGD(lr)\n",
        "trainer = RnnlmTrainer(model, optimizer)\n",
        "\n",
        "trainer.fit(xs, ts, max_epoch, batch_size, time_size)\n",
        "trainer.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeH3J8Qt9_Bs"
      },
      "source": [
        "##### 9. 다음 RNN 계층에서 도출된 수식을 설명하세요. $$\\mathbf{h}_{t} = \\tanh{\\left( \\mathbf{h}_{t-1} \\mathbf{W}_{\\mathbf{h}} + \\mathbf{x}_{t} \\mathbf{W}_{\\mathbf{x}} + \\mathbf{b} \\right)}$$\n",
        "##### "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd9sIFZK9_Bs"
      },
      "source": [
        "- 답안\n",
        "RNN에서 사용하는 가중치 2개: Wx (입력 x를 출력 h 로 변환하기 위한 가중치), Wh (1개의 RNN 출력을 다음 시각의 출력으로 변환하기 위한 가중치)\n",
        "\n",
        "편향 b\n",
        "\n",
        "행벡터: ht-1, xt\n",
        "\n",
        "먼저 행렬 곱을 계산하고, 그 합을 tanh 함수를 이용해 변환하면 결과는 시각 t의 출력 ht가 도출된다. ht는 한 시각 이전 출력인 ht-1에 기초해 계산된다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewLxiw-i9_Bs"
      },
      "source": [
        "##### 10. Truncated BPTT 과정에서 유의해야 할 점이 무엇인지 서술하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnhUAxds9_Bt"
      },
      "source": [
        "- 답안\n",
        ": Truncated BPTT는 길어진 신경망을 적당한 지점에서 작은 신경망 여러 개로 끊어주는 기법을 사용한 것으로, 순전파의 연결은 그대로 유지한 채 역전파의 연결만 끊어줘야 한다는 점을 주의해야줘야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv-YejoK9_Bt"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xNKZkQH9_Bt"
      },
      "source": [
        "#### 11. (추가 예제) https://wikidocs.net/45101 (딥 러닝을 이용한 자연어처리 입문)의\n",
        "##### 08. 순환 신경망\n",
        " - ##### 6) RNN을 이용한 텍스트 생성 예제\n",
        "   + ##### 1. <RNN을 이용하여 텍스트 생성하기>를 체험해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2YASgsy9_Bt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# text 내용을 바꿔도 좋습니다.\n",
        "# 호랑이도 제말하면 온다.\n",
        "# 이빨 빠진 호랑이\n",
        "# 호랑이 굴에 들어가도 정신만 차리면 산다\n",
        "\n",
        "text = \"\"\"경마장에 있는 말이 뛰고 있다\\n\n",
        "그의 말이 법이다\\n\n",
        "가는 말이 고와야 오는 말이 곱다\\n\"\"\"\n",
        "\n",
        "#code"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "KHUDA_Homework3_최예지.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}